{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichalMSlusarski/ML-semestr-letni/blob/main/lda_test_development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download pl_core_news_sm"
      ],
      "metadata": {
        "id": "VSB1bULi3d6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "import string\n",
        "import spacy\n",
        "import en_core_web_sm #??\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import re \n"
      ],
      "metadata": {
        "id": "jad9GaRy3gSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "fXGjAqe3PIwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('articles_reduced.csv')\n",
        "nlp = spacy.load(\"pl_core_news_sm\")\n",
        "\n",
        "PUNCTUATION = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~“”’–„'''\n",
        "STOP_WORDS = ['moment', 'chodzić', 'robić', 'wrażenie', 'faktycznie', 'myśleć', 'chcieć', 'czas', 'zrobić', 'wiedzieć', 'trochę', 'swój', 'super', 'naprawdę','prosty', 'drugi', 'przykład', 'troszeczkę', 'raczej', 'siebie', 'wydawać','rzecz', 'chyba', 'różny', 'a',\t'aby',\t'ach',\t'acz',\t'aczkolwiek',\t'aj',\t'albo',\t'ale',\t'ależ',\t'ani',\t'aż',\t'bardziej',\t'bardzo',\t'bez',\t'bo',\t'bowiem',\t'by',\t'byli',\t'bym',\t'bynajmniej',\t'być',\t'był',\t'była',\t'było',\t'były',\t'będzie',\t'będą',\t'cali',\t'cała',\t'cały',\t'chce',\t'choć',\t'ci',\t'ciebie',\t'cię',\t'co',\t'cokolwiek',\t'coraz',\t'coś',\t'czasami',\t'czasem',\t'czemu',\t'czy',\t'czyli',\t'często',\t'daleko',\t'dla',\t'dlaczego',\t'dlatego',\t'do',\t'dobrze',\t'dokąd',\t'dość',\t'dr',\t'dużo',\t'dwa',\t'dwaj',\t'dwie',\t'dwoje',\t'dzisiaj',\t'dziś',\t'gdy',\t'gdyby',\t'gdyż',\t'gdzie',\t'gdziekolwiek',\t'gdzieś',\t'go',\t'godz',\t'hab',\t'i',\t'ich',\t'ii',\t'iii',\t'ile',\t'im',\t'inna',\t'inne',\t'inny',\t'innych',\t'inż',\t'iv',\t'ix',\t'iż',\t'ja',\t'jak',\t'jakaś',\t'jakby',\t'jaki',\t'jakichś',\t'jakie',\t'jakiś',\t'jakiż',\t'jakkolwiek',\t'jako',\t'jakoś',\t'je',\t'jeden',\t'jedna',\t'jednak',\t'jednakże',\t'jedno',\t'jednym',\t'jedynie',\t'jego',\t'jej',\t'jemu',\t'jest',\t'jestem',\t'jeszcze',\t'jeśli',\t'jeżeli',\t'już',\t'ją',\t'każdy',\t'kiedy',\t'kierunku',\t'kilka',\t'kilku',\t'kimś',\t'kto',\t'ktokolwiek',\t'ktoś',\t'która',\t'które',\t'którego',\t'której',\t'który',\t'których',\t'którym',\t'którzy',\t'ku',\t'lat',\t'lecz',\t'lub',\t'ma',\t'mają',\t'mam',\t'mamy',\t'mało',\t'mgr',\t'mi',\t'miał',\t'mimo',\t'między',\t'mnie',\t'mną',\t'mogą',\t'moi',\t'moim',\t'moja',\t'moje','móc', 'mieć',\t'może',\t'możliwe',\t'można',\t'mu',\t'musi',\t'my',\t'mój',\t'na',\t'nad',\t'nam',\t'nami',\t'nas',\t'nasi',\t'nasz',\t'nasza',\t'nasze',\t'naszego',\t'naszych',\t'natomiast',\t'natychmiast',\t'nawet',\t'nic',\t'nich',\t'nie',\t'niech',\t'niego',\t'niej',\t'niemu',\t'nigdy',\t'nim',\t'nimi',\t'nią',\t'niż',\t'no',\t'nowe',\t'np',\t'nr',\t'o',\t'o.o.',\t'obok',\t'od',\t'ok',\t'około',\t'on',\t'ona',\t'one',\t'oni',\t'ono',\t'oraz',\t'oto',\t'owszem',\t'pan',\t'pana',\t'pani',\t'pl',\t'po',\t'pod',\t'podczas',\t'pomimo',\t'ponad',\t'ponieważ',\t'powinien',\t'powinna',\t'powinni',\t'powinno',\t'poza',\t'prawie',\t'prof',\t'przecież',\t'przed',\t'przede',\t'przedtem',\t'przez',\t'przy',\t'raz',\t'razie',\t'roku',\t'również',\t'sam',\t'sama',\t'się',\t'skąd',\t'sobie',\t'sobą',\t'sposób',\t'swoje',\t'są',\t'ta',\t'tak',\t'taka',\t'taki',\t'takich',\t'takie',\t'także',\t'tam',\t'te',\t'tego',\t'tej',\t'tel',\t'temu',\t'ten',\t'teraz',\t'też',\t'to',\t'tobie',\t'tobą',\t'toteż',\t'totobą',\t'trzeba',\t'tu',\t'tutaj',\t'twoi',\t'twoim',\t'twoja',\t'twoje',\t'twym',\t'twój',\t'ty',\t'tych',\t'tylko',\t'tym',\t'tys',\t'tzw',\t'tę',\t'u',\t'ul',\t'vi',\t'vii',\t'viii',\t'vol',\t'w',\t'wam',\t'wami',\t'was',\t'wasi',\t'wasz',\t'wasza',\t'wasze',\t'we',\t'według',\t'wie',\t'wiele',\t'wielu',\t'więc',\t'więcej',\t'wszyscy',\t'wszystkich',\t'wszystkie',\t'wszystkim',\t'wszystko',\t'wtedy',\t'www',\t'wy',\t'właśnie',\t'wśród',\t'xi',\t'xii',\t'xiii',\t'xiv',\t'xv',\t'z',\t'za',\t'zapewne',\t'zawsze',\t'zaś',\t'ze',\t'zeznowu',\t'znowu',\t'znów',\t'został',\t'zł',\t'żaden',\t'żadna',\t'żadne',\t'żadnych',\t'że',\t'żeby']\n",
        "\n",
        "lemma = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "W67LCo2lPFkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions"
      ],
      "metadata": {
        "id": "LIKGCyQSPMua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GEeWybx2Ss8"
      },
      "outputs": [],
      "source": [
        "def prepare_sentences(text):\n",
        "    sentences = [] \n",
        "    for sentence in text.replace('\\n','').split('.'):\n",
        "        if len(sentence) > 10:\n",
        "            sentences.append(sentence.strip())\n",
        "    return sentences\n",
        "\n",
        "def clean(doc):\n",
        "    \n",
        "    words = \" \".join([i for i in doc.lower().split()])\n",
        "    # words = words.translate(str.maketrans('', '', string.punctuation))\n",
        "    words = nlp(words)\n",
        "    \n",
        "    normalized_list = []\n",
        "    for word in words:\n",
        "        lemma = lemmatize_word(word)\n",
        "        if lemma not in STOP_WORDS and lemma not in PUNCTUATION and ' ' not in lemma and lemma:\n",
        "            normalized_list.append(lemma)\n",
        "\n",
        "    normalized = ' '.join(normalized_list)  \n",
        "\n",
        "    # print(\"NORMALIZED: \" + normalized) #testuj tu cos                      \n",
        "    return normalized #zwraca znormalizwane zdanie po zdaniu \n",
        "\n",
        "def lemmatize_word(word):\n",
        "    lemma = word.lemma_\n",
        "    if lemma.strip().lower():\n",
        "        return lemma\n",
        "    \n",
        "def get_lda(text, topics, words):\n",
        "    doc_complete = prepare_sentences(text)\n",
        "    \n",
        "    doc_clean = [clean(doc).split() for doc in doc_complete] \n",
        "\n",
        "    # Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
        "    dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
        "    doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "\n",
        "    # Creating the object for LDA model using gensim library\n",
        "    Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "    # Running and Trainign LDA model on the document term matrix.\n",
        "    ldamodel = Lda(doc_term_matrix, num_topics=topics, id2word = dictionary, passes=50)\n",
        "    lda = ldamodel.print_topics(num_topics=topics, num_words=words)\n",
        "\n",
        "    return lda\n",
        "\n",
        "def topic_modeling(data, content_col, label_col):\n",
        "\n",
        "    topic_list = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterating through content with topic modeling\n",
        "    for n,article in enumerate(tqdm(data[content_col])):\n",
        "        if type(article) == str and 'sport' not in article.lower():\n",
        "            try:\n",
        "                topics = get_lda(article, 3, 10)\n",
        "                topic_list.append(topics)\n",
        "\n",
        "                labels.append(list(data[label_col])[n])\n",
        "            except:\n",
        "                pass\n",
        "            \n",
        "    return [topic_list, labels]\n",
        "\n",
        "def generate_keywords(topic_list):\n",
        "    keywords = []\n",
        "\n",
        "    # Extract the topic keywords without the likelihoods\n",
        "    for t_list in topic_list:\n",
        "        keywords.append([fragment for tup in t_list for fragment in re.findall(r'\"([^\"]*)\"', tup[1])])\n",
        "        \n",
        "    return keywords\n",
        "\n",
        "\n",
        "def generate_similarity_matrix(document_list): \n",
        "    global vectorizer \n",
        "\n",
        "    # Create the TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer()\n",
        "\n",
        "    # Compute the TF-IDF matrix\n",
        "    tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "\n",
        "    # Calculate the cosine similarity matrix\n",
        "    cosine_similarities = cosine_similarity(tfidf_matrix)\n",
        "    \n",
        "    return cosine_similarities\n",
        "    \n",
        "def generate_similarity_rank(cosine_similarities, labels, min_strength):\n",
        "    data = pd.DataFrame()\n",
        "    # Print the cosine similarity matrix\n",
        "    for i in tqdm(range(len(cosine_similarities))):\n",
        "        for j in range(i + 1, len(cosine_similarities)):\n",
        "            # print(f\"Similarity between list {i} and list {j}: {cosine_similarities[i][j]}\")\n",
        "            tmp = pd.DataFrame()\n",
        "            if cosine_similarities[i][j] > min_strength and labels[i] != labels[j]:\n",
        "                tmp['content_1'] = [labels[i]]\n",
        "                tmp['content_2'] = [labels[j]]\n",
        "                tmp['similarity'] = [cosine_similarities[i][j]]\n",
        "                data = data.append(tmp)\n",
        "                \n",
        "    data = data.sort_values(by=['similarity'], ascending = False)\n",
        "\n",
        "    return data\n",
        "\n",
        "def find_similar_content(cosine_similarities, labels, content, min_strength):\n",
        "    data = pd.DataFrame()\n",
        "    \n",
        "    for i in range(len(cosine_similarities)):\n",
        "\n",
        "        # Temporary dataframe \n",
        "        tmp = pd.DataFrame()\n",
        "\n",
        "        for j in range(i + 1, len(cosine_similarities)):\n",
        "\n",
        "            if cosine_similarities[i][j] > min_strength and labels[i] == content:\n",
        "                tmp['content_1'] = [labels[i]]\n",
        "                tmp['content_2'] = [labels[j]]\n",
        "                tmp['similarity'] = [cosine_similarities[i][j]]\n",
        "\n",
        "                data = data.append(tmp)\n",
        "\n",
        "    data = data.sort_values(by = ['similarity'], ascending = False)\n",
        "    \n",
        "    return data\n",
        "\n",
        "def cluster_data(document_list, number_of_clusters):\n",
        "    # Fit and transform the vectorizer on the document list\n",
        "    vectorized_docs = vectorizer.fit_transform(document_list)\n",
        "\n",
        "    # Apply dimensionality reduction to reduce the vectorized documents to two or three dimensions\n",
        "    pca = PCA(n_components=2)  # or n_components=3 for 3D visualization\n",
        "    reduced_docs = pca.fit_transform(vectorized_docs.toarray())\n",
        "\n",
        "    # Perform K-means clustering\n",
        "    kmeans = KMeans(n_clusters=number_of_clusters)\n",
        "    cluster_labels = kmeans.fit_predict(reduced_docs)\n",
        "    \n",
        "    return [reduced_docs, cluster_labels]\n",
        "\n",
        "def generate_clustered_table(reduced_docs, labels, cluster_labels):\n",
        "    data = pd.DataFrame()\n",
        "\n",
        "    for n, loc in enumerate(reduced_docs):\n",
        "        tmp = pd.DataFrame()\n",
        "        \n",
        "        loc = str(loc).replace('[','').replace(']','').strip().replace('  ', ' ').replace('  ', ' ')\n",
        "        tmp['x'] = [loc.split(' ')[0]]\n",
        "        tmp['y'] = [loc.split(' ')[1]]\n",
        "        tmp['content'] = [labels[n]]\n",
        "        tmp['topic'] = [cluster_labels[n]]\n",
        "        \n",
        "        data = data.append(tmp)\n",
        "    \n",
        "    data = data.sort_values(by=['topic'])\n",
        "    return data\n",
        "\n",
        "def draw_viz(reduced_docs, cluster_labels):\n",
        "    # Visualize the clustered data\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    scatter = ax.scatter(reduced_docs[:, 0], reduced_docs[:, 1], c=cluster_labels, cmap='BuPu')\n",
        "\n",
        "    plt.title('Clustered Documents')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.colorbar(scatter)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def draw_viz_with_topics(reduced_docs, cluster_labels, topics):\n",
        "    # Visualize the clustered data with topics\n",
        "    plt.style.use('dark_background')\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 20))\n",
        "    scatter = ax.scatter(reduced_docs[:, 0], reduced_docs[:, 1], c=cluster_labels, cmap='BuPu')\n",
        "\n",
        "    # Plotting the topic information\n",
        "    for i, txt in enumerate(topics):\n",
        "        ax.annotate(txt, (reduced_docs[i, 0], reduced_docs[i, 1]), fontsize=8, ha='center')\n",
        "\n",
        "    plt.title('Clustered Documents with Topics')\n",
        "    plt.xlabel('Dimension 1')\n",
        "    plt.ylabel('Dimension 2')\n",
        "    plt.colorbar(scatter)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def elbow_method():\n",
        "    max_clusters = 10\n",
        "    wcss = elbow_method(reduced_docs, max_clusters)\n",
        "    plt.plot(range(1, max_clusters + 1), wcss, marker='o')\n",
        "    plt.title('Elbow Method')\n",
        "    plt.xlabel('Number of Clusters')\n",
        "    plt.ylabel('WCSS')\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology tests"
      ],
      "metadata": {
        "id": "sSkz_epEPTdH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data and variables"
      ],
      "metadata": {
        "id": "HY3dBSGCQXQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = topic_modeling(data, 'content', 'title')\n",
        "topic_list, labels = output[0], output[1]"
      ],
      "metadata": {
        "id": "EGLiGlUaMm1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = generate_keywords(topic_list)\n",
        "#print(keywords) \n",
        "# Convert each list of words into a string\n",
        "document_list = [' '.join(words) for words in keywords]\n",
        "sim_matrix = generate_similarity_matrix(document_list)\n",
        "\n",
        "output = cluster_data(document_list, 8)\n",
        "reduced_docs, cluster_labels = output[0], output[1]"
      ],
      "metadata": {
        "id": "GSeHOlHRPAyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating similarity rank"
      ],
      "metadata": {
        "id": "dA5aT3rtQbuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sim_rank = generate_similarity_rank(sim_matrix, labels, 0.2)\n",
        "sim_rank"
      ],
      "metadata": {
        "id": "FX6xhP2e28Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find similar content"
      ],
      "metadata": {
        "id": "qvnRDHSuQgWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "title = 'Nie okazuj emocji i nie proś o pomoc. O stereotypach dotyczących męskiego zdrowia psychicznego (ROZMOWA)'\n",
        "similars = find_similar_content(sim_matrix, labels, title, 0.1)\n",
        "similars"
      ],
      "metadata": {
        "id": "RK0vRD9u28KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustered table"
      ],
      "metadata": {
        "id": "VPls1FlrQqvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clustered_table = generate_clustered_table(reduced_docs, labels, cluster_labels)\n",
        "clustered_table"
      ],
      "metadata": {
        "id": "RSQMMv7e28PH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clustered viz"
      ],
      "metadata": {
        "id": "5HWtL4qNQt76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw_viz(reduced_docs, cluster_labels)\n",
        "draw_viz_with_topics(reduced_docs, cluster_labels, keywords)\n",
        "#loop through clusters\n",
        "\n",
        "for i in range(5, 15):\n",
        "  output = cluster_data(document_list, i)\n",
        "  reduced_docs, cluster_labels = output[0], output[1]\n",
        "  draw_viz_with_topics(reduced_docs, cluster_labels, keywords)\n",
        "  #calculate the mean square root sum (elbow)"
      ],
      "metadata": {
        "id": "eY6WEKHj28Rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}